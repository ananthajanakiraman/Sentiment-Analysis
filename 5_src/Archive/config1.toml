stop-words = "../../meta/data/lemur-stopwords.txt"
libsvm-modules = "../../meta/deps/libsvm-modules/"
prefix = "../../meta/data/"
function-words = "../../meta/data/function-words.txt"
punctuation = "../../meta/data/sentence-boundaries/sentence-punctuation.txt"
start-exceptions = "../../meta/data/sentence-boundaries/sentence-start-exceptions.txt"
end-exceptions = "../../meta/data/sentence-boundaries/sentence-end-exceptions.txt"
datafile-path = "../../meta/data/sentiment/"
datafile-name = "sentiment.dat"

corpus = "line.toml"
dataset = "sentiment"
forward-index = "sentiment-fwd"
index = "sentiment-inv"

[[analyzers]]
method = "ngram-word"
ngram = 1
filter = "default-chain"
#	[[analyzers.filter]]
#	type = "whitespace-tokenizer"
#	[[analyzers.filter]]
#	type = "lowercase"
#	[[analyzers.filter]]
#	type = "alpha"
#	[[analyzers.filter]]
#  	type = "length"
#	min = 2
#  	max = 35
#	[[analyzers.filter]]
#	type = "list"
#	file = "../../meta/data/lemur-stopwords.txt"
#	[[analyzers.filter]]
#	type = "porter2-filter"

[ranker]
method = "bm25"
k1 = 1.2
b = 0.75
k3 = 500

[classifier]
method = "naive-bayes"
#[classifier.base]
#method = "sgd"
#loss = "hinge"

[lda]
inference = "gibbs"
max-iters = 1000
alpha = 1.0
beta = 1.0
topics = 4
model-prefix = "lda-model"

[crf]
prefix = "crf"
treebank = "penn-treebank" # relative to data prefix
corpus = "wsj"
section-size = 99
train-sections = [0, 18]
dev-sections = [19, 21]
test-sections = [22, 24]

[diff]
prefix = "../../meta/data"
dataset = "20newsgroups"
n-value = 3
max-edits = 3
# penalty defaults are all zero (no penalty)
base-penalty = 0.0 # base penalty is for any edit
insert-penalty = 0.0
substitute-penalty = 0.0
remove-penalty = 0.0

[features]
method = "info-gain"
prefix = "features"
features-per-class = 20

[sequence]
prefix = "../../meta/data/perceptron-tagger"
treebank = "penn-treebank" # relative to data prefix
corpus = "wsj"
section-size = 99
train-sections = [0, 18]
dev-sections = [19, 21]
test-sections = [22, 24]

[parser]
prefix = "parser"
treebank = "penn-treebank" # relative to data prefix
corpus = "wsj"
section-size = 99
train-sections = [2, 21]
dev-sections = [22, 22]
test-sections = [23, 23]

[language-model]
arpa-file = "../../meta/data/english-sentences.arpa"
binary-file-prefix = "english-sentences-"

[embeddings]
prefix = "word-embeddings"
filter = [{type = "icu-tokenizer"}, {type = "lowercase"}]
vector-size = 50
[embeddings.vocab]
min-count = 10
max-size = 500000
